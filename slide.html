<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Efficient Attention Mechanisms in LLMs</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&family=JetBrains+Mono:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <style>
        :root {
            --bg-color: #0f172a; /* Dark Slate */
            --card-bg: #1e293b; /* Lighter Slate */
            --text-primary: #f8fafc; /* White/Light Grey */
            --text-secondary: #94a3b8; /* Muted Grey */
            --accent-primary: #38bdf8; /* Sky Blue */
            --accent-secondary: #4ade80; /* Green */
            --accent-tertiary: #f472b6; /* Pink for contrast */
            --font-main: 'Inter', sans-serif;
            --font-mono: 'JetBrains Mono', monospace;
        }

        * { box-sizing: border-box; }

        body {
            background-color: #020617; /* Very dark bg for presentation */
            margin: 0;
            height: 100vh;
            width: 100vw;
            display: flex;
            justify-content: center;
            align-items: center;
            font-family: var(--font-main);
            overflow: hidden; /* Prevent scrolling */
        }

        /* CORE SLIDE CONTAINER */
        .slide-container {
            background-color: var(--bg-color);
            border-radius: 16px;
            box-shadow: 0 0 50px rgba(0, 0, 0, 0.8);
            display: none; /* Hidden by default */
            flex-direction: column;
            height: 720px;
            width: 1280px;
            overflow: hidden;
            padding: 60px;
            position: relative;
            color: var(--text-primary);
            /* Scale down on smaller screens */
            transform-origin: center center;
        }

        /* Active Slide Class */
        .slide-container.active {
            display: flex;
            animation: fadeIn 0.4s ease-in-out;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: scale(0.98); }
            to { opacity: 1; transform: scale(1); }
        }

        /* Background Aesthetic */
        .slide-container::before {
            content: '';
            position: absolute;
            top: -50%; left: -20%; width: 80%; height: 80%;
            background: radial-gradient(circle, rgba(56, 189, 248, 0.08) 0%, transparent 60%);
            filter: blur(60px); z-index: 0;
        }

        .slide-container::after {
            content: '';
            position: absolute;
            bottom: -30%; right: -10%; width: 60%; height: 60%;
            background: radial-gradient(circle, rgba(74, 222, 128, 0.05) 0%, transparent 60%);
            filter: blur(60px); z-index: 0;
        }

        .slide-container > * { position: relative; z-index: 1; }

        /* CONTROLS */
        .controls {
            position: fixed;
            bottom: 20px;
            right: 20px;
            display: flex;
            gap: 10px;
            z-index: 1000;
            background: rgba(255, 255, 255, 0.1);
            padding: 10px;
            border-radius: 50px;
            backdrop-filter: blur(5px);
        }

        .btn {
            background: transparent;
            border: 1px solid rgba(255,255,255,0.2);
            color: white;
            padding: 10px 15px;
            border-radius: 50%;
            cursor: pointer;
            transition: all 0.2s;
            font-size: 16px;
        }

        .btn:hover { background: rgba(255,255,255,0.2); transform: scale(1.1); }
        
        .slide-counter {
            position: fixed;
            bottom: 30px;
            left: 30px;
            color: var(--text-secondary);
            font-family: var(--font-mono);
            font-size: 14px;
            z-index: 1000;
        }

        /* TYPOGRAPHY */
        h1, h2, h3 { margin: 0; font-weight: 700; letter-spacing: -0.02em; }

        h1 {
            font-size: 72px;
            background: linear-gradient(135deg, var(--text-primary), var(--accent-primary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 20px;
        }

        .slide-title {
            font-size: 42px;
            color: var(--accent-primary);
            margin-bottom: 40px;
            border-bottom: 1px solid rgba(255,255,255,0.1);
            padding-bottom: 20px;
            width: 100%;
        }

        p, li {
            font-size: 22px;
            line-height: 1.6;
            color: var(--text-secondary);
            margin-bottom: 15px;
        }

        strong { color: var(--text-primary); font-weight: 600; }

        /* LAYOUT UTILS */
        .content-area {
            display: flex; flex-direction: column; flex-grow: 1;
            justify-content: center; width: 100%;
        }

        .two-column {
            display: grid; grid-template-columns: 1fr 1fr;
            gap: 60px; align-items: center; height: 100%;
        }

        .diagram-wrapper {
            border-radius: 12px; overflow: hidden;
            box-shadow: 0 8px 24px rgba(0,0,0,0.3);
            border: 1px solid rgba(255,255,255,0.1);
            height: 100%; max-height: 450px;
            display: flex; justify-content: center; align-items: center;
            background: #151e2e; padding: 20px;
        }

        /* TILED LAYOUTS */
        .tiled-content {
            display: grid; grid-template-columns: repeat(4, 1fr);
            gap: 30px; width: 100%;
        }

        .tile {
            background: var(--card-bg); padding: 30px;
            border-radius: 12px; border: 1px solid rgba(255,255,255,0.05);
            text-align: center; display: flex; flex-direction: column;
            align-items: center;
        }

        .tile .icon { font-size: 48px; color: var(--accent-secondary); margin-bottom: 20px; }
        .tile h3 { font-size: 20px; color: var(--text-primary); margin-bottom: 10px; }
        .tile p { font-size: 16px; line-height: 1.4; }

        /* CHARTS */
        .chart-container {
            display: flex; flex-direction: column; gap: 25px;
            width: 100%; padding: 20px 0;
        }

        .bar-row { display: flex; align-items: center; gap: 20px; }
        .bar-label { flex: 0 0 240px; text-align: right; font-size: 18px; font-weight: 600; color: var(--text-primary); }
        .bar-track { flex-grow: 1; background: rgba(255,255,255,0.1); border-radius: 6px; height: 40px; overflow: hidden; }
        .bar-fill {
            height: 100%; background: linear-gradient(90deg, var(--accent-primary), var(--accent-secondary));
            display: flex; align-items: center; justify-content: flex-end;
            padding-right: 15px; color: #0f172a; font-weight: 700; font-size: 16px; white-space: nowrap;
        }

        /* TABLE */
        table { width: 100%; border-collapse: collapse; margin-top: 20px; }
        th { text-align: left; padding: 15px; color: var(--accent-primary); border-bottom: 2px solid rgba(255,255,255,0.1); font-size: 20px; }
        td { padding: 15px; border-bottom: 1px solid rgba(255,255,255,0.05); color: var(--text-secondary); font-size: 18px; }

        /* MATHML */
        math { font-family: var(--font-mono); color: var(--accent-secondary); font-size: 1.1em; }

        /* BULLETS */
        .styled-list ul { list-style: none; padding: 0; }
        .styled-list li { padding-left: 40px; position: relative; margin-bottom: 25px; }
        .styled-list li::before {
            content: '\f058'; font-family: 'Font Awesome 6 Free'; font-weight: 900;
            color: var(--accent-secondary); position: absolute; left: 0; top: 4px; font-size: 20px;
        }

        /* FULL BG */
        .full-bg-slide {
            position: relative; overflow: hidden;
            justify-content: center; align-items: center; text-align: center;
            background: radial-gradient(circle at center, #1e293b 0%, #0f172a 100%);
        }
        
        .full-bg-slide::before {
            content: ''; position: absolute; top: 0; left: 0; right: 0; bottom: 0;
            background-image: radial-gradient(rgba(56, 189, 248, 0.1) 1px, transparent 1px);
            background-size: 40px 40px; z-index: 0;
        }
        
        .full-bg-overlay {
            background: rgba(15, 23, 42, 0.85); padding: 60px;
            border-radius: 16px; backdrop-filter: blur(10px); max-width: 800px;
        }
    </style>
</head>
<body>

<!-- CONTROLS -->
<div class="slide-counter" id="counter">1 / 12</div>
<div class="controls">
    <button class="btn" onclick="prevSlide()" title="Previous (Left Arrow)"><i class="fa-solid fa-chevron-left"></i></button>
    <button class="btn" onclick="nextSlide()" title="Next (Right Arrow)"><i class="fa-solid fa-chevron-right"></i></button>
    <button class="btn" onclick="toggleFullScreen()" title="Fullscreen (F)"><i class="fa-solid fa-expand"></i></button>
</div>

<!-- Slide 1: Title Slide -->
<div class="slide-container active" id="slide1">
    <div class="content-area" style="text-align: center; align-items: center;">
        <h1>Efficient Attention Mechanisms<br>for Large Language Models</h1>
        <p style="font-size: 24px; max-width: 800px; margin: 20px auto;">From quadratic bottlenecks to hardware-aware, sparse, and low-rank solutions.</p>
        <div style="margin-top: 50px; border-top: 1px solid rgba(255,255,255,0.1); padding-top: 20px; display: inline-block;">
            <p style="font-size: 18px;">November 23, 2025</p>
        </div>
    </div>
</div>

<!-- Slide 2: The Problem -->
<div class="slide-container" id="slide2">
    <h2 class="slide-title">The Quadratic Bottleneck</h2>
    <div class="content-area">
        <div class="two-column">
            <div>
                <h3>Scaling Limits</h3>
                <p>The core limitation is the self-attention mechanism cost, scaling quadratically with sequence length <math><mi>N</mi></math>.</p>
                <ul>
                    <li><strong>Compute Cost:</strong> <math><mi>O</mi><mfenced><msup><mi>N</mi><mn>2</mn></msup><mi>d</mi></mfenced></math> FLOPs.</li>
                    <li><strong>Memory Cost:</strong> Storing scores requires <math><mi>O</mi><mfenced><msup><mi>N</mi><mn>2</mn></msup></mfenced></math>.</li>
                    <li><strong>KV Cache:</strong> Grows linearly <math><mi>O</mi><mfenced><mrow><mi>N</mi><mi>L</mi><mi>d</mi></mrow></mfenced></math>. For 128K context, this can exceed HBM capacity.</li>
                </ul>
            </div>
            <!-- DIAGRAM: Quadratic vs Linear Graph -->
            <div class="diagram-wrapper">
                <svg viewBox="0 0 400 300" width="100%" height="100%" style="font-family: var(--font-mono);">
                    <!-- Axes -->
                    <line x1="50" y1="250" x2="350" y2="250" stroke="#94a3b8" stroke-width="2" />
                    <line x1="50" y1="250" x2="50" y2="50" stroke="#94a3b8" stroke-width="2" />
                    <text x="320" y="280" fill="#94a3b8" font-size="14">Sequence Length (N)</text>
                    <text x="20" y="40" fill="#94a3b8" font-size="14" transform="rotate(-90 40 40)">Cost / Latency</text>
                    
                    <!-- Quadratic Curve (N^2) -->
                    <path d="M 50 250 Q 200 240 300 50" fill="none" stroke="#ef4444" stroke-width="4" />
                    <text x="310" y="50" fill="#ef4444" font-size="16" font-weight="bold">O(N²)</text>
                    
                    <!-- Linear Line (N) -->
                    <line x1="50" y1="250" x2="350" y2="200" stroke="#4ade80" stroke-width="4" stroke-dasharray="8,4"/>
                    <text x="360" y="200" fill="#4ade80" font-size="16" font-weight="bold">Goal: O(N)</text>

                    <!-- Limit Line -->
                    <line x1="250" y1="250" x2="250" y2="50" stroke="#f472b6" stroke-width="2" stroke-dasharray="4"/>
                    <text x="180" y="80" fill="#f472b6" font-size="12">Memory Wall</text>
                </svg>
            </div>
        </div>
    </div>
</div>

<!-- Slide 3: Overview of Approaches -->
<div class="slide-container" id="slide3">
    <h2 class="slide-title">The Efficiency Landscape</h2>
    <div class="content-area">
        <div class="tiled-content">
            <div class="tile">
                <div class="icon"><i class="fa-solid fa-microchip"></i></div>
                <h3>FlashAttention-3</h3>
                <p><strong>Hardware Co-design</strong><br>Exact computation optimized for Hopper GPUs (TMA & Asynchrony).</p>
            </div>
            <div class="tile">
                <div class="icon"><i class="fa-solid fa-filter"></i></div>
                <h3>PSA</h3>
                <p><strong>Adaptive Sparsity</strong><br>Progressive Sparse Attention tailored for serving inference.</p>
            </div>
            <div class="tile">
                <div class="icon"><i class="fa-solid fa-calculator"></i></div>
                <h3>HyperAttention</h3>
                <p><strong>Theoretical Approx</strong><br>Near-linear <math><mi>O</mi><mfenced><mrow><mi>N</mi><mi>log</mi><mi>N</mi></mrow></mfenced></math> via LSH hashing.</p>
            </div>
            <div class="tile">
                <div class="icon"><i class="fa-solid fa-brain"></i></div>
                <h3>LoRA-Sparse</h3>
                <p><strong>Learned Efficiency</strong><br>Low-rank adaptation to predict sparsity patterns.</p>
            </div>
        </div>
    </div>
</div>

<!-- Slide 4: FlashAttention-3 -->
<div class="slide-container" id="slide4">
    <h2 class="slide-title">FlashAttention-3: Exact & Hardware-Aware</h2>
    <div class="content-area">
        <div class="two-column">
            <div>
                <h3>Hopper-Optimized Design</h3>
                <p>Leverages NVIDIA H100 features to hide memory latency completely.</p>
                <ul>
                    <li><strong>Warp Specialization:</strong> Separates producer warps (TMA loading) from consumer warps (GEMM).</li>
                    <li><strong>Asynchrony:</strong> Overlaps data movement from HBM to SRAM with computation.</li>
                    <li><strong>FP8 Precision:</strong> Uses block quantization and incoherent transforms to stabilize low-precision math.</li>
                </ul>
            </div>
            <!-- DIAGRAM: Memory Hierarchy & Tiling -->
            <div class="diagram-wrapper">
                <svg viewBox="0 0 400 300" width="100%" height="100%" style="font-family: var(--font-mono);">
                    <!-- HBM Block -->
                    <rect x="20" y="50" width="80" height="200" rx="8" fill="#1e293b" stroke="#38bdf8" stroke-width="2"/>
                    <text x="60" y="150" fill="#38bdf8" text-anchor="middle" font-weight="bold" transform="rotate(-90 60 150)">HBM (High Memory)</text>
                    
                    <!-- SRAM Block -->
                    <rect x="180" y="100" width="80" height="100" rx="8" fill="#1e293b" stroke="#4ade80" stroke-width="2"/>
                    <text x="220" y="155" fill="#4ade80" text-anchor="middle" font-weight="bold">SRAM</text>
                    
                    <!-- Compute Block -->
                    <rect x="300" y="100" width="80" height="100" rx="8" fill="#1e293b" stroke="#f472b6" stroke-width="2"/>
                    <text x="340" y="155" fill="#f472b6" text-anchor="middle" font-weight="bold">Tensor Core</text>

                    <!-- Arrows / Flow -->
                    <path d="M 100 120 L 170 120" stroke="#94a3b8" stroke-width="3" marker-end="url(#arrowhead)"/>
                    <text x="135" y="110" fill="#94a3b8" font-size="10" text-anchor="middle">TMA Load</text>

                    <path d="M 260 150 L 290 150" stroke="#94a3b8" stroke-width="3" marker-end="url(#arrowhead)"/>
                    
                    <path d="M 100 180 L 170 180" stroke="#ef4444" stroke-width="2" stroke-dasharray="4"/>
                    <text x="135" y="200" fill="#ef4444" font-size="10" text-anchor="middle">Asynch Overlap</text>

                    <!-- Definition of Arrowhead -->
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" fill="#94a3b8" />
                        </marker>
                    </defs>
                </svg>
            </div>
        </div>
    </div>
</div>

<!-- Slide 5: Progressive Sparse Attention -->
<div class="slide-container" id="slide5">
    <h2 class="slide-title">PSA: System-Level Inference</h2>
    <div class="content-area">
        <div class="two-column">
            <!-- DIAGRAM: Sparse Matrix Pattern -->
            <div class="diagram-wrapper">
                 <svg viewBox="0 0 300 300" width="100%" height="100%">
                    <defs>
                        <pattern id="grid" width="30" height="30" patternUnits="userSpaceOnUse">
                            <rect width="30" height="30" fill="none" stroke="#334155" stroke-width="1"/>
                        </pattern>
                    </defs>
                    <rect width="300" height="300" fill="url(#grid)" />
                    
                    <!-- Diagonal (Dense) -->
                    <path d="M0 0 L300 300 L300 240 L60 0 Z" fill="#38bdf8" opacity="0.8"/>
                    
                    <!-- Heavy Hitters (Scattered) -->
                    <rect x="0" y="120" width="30" height="30" fill="#4ade80" />
                    <rect x="0" y="210" width="30" height="30" fill="#4ade80" />
                    <rect x="90" y="240" width="30" height="30" fill="#4ade80" />
                    
                    <!-- Empty areas represent sparsity -->
                    <text x="200" y="80" fill="#94a3b8" font-family="var(--font-mono)" font-size="14">Pruned / Sparse</text>
                    <text x="80" y="150" fill="#fff" font-family="var(--font-mono)" font-size="14" font-weight="bold">Active KV Blocks</text>
                 </svg>
            </div>
            <div>
                <h3>Solving the Memory Wall</h3>
                <p>PSA focuses on the inference phase where KV cache is the bottleneck.</p>
                <ul>
                    <li><strong>Progressive Thresholding:</strong> Stops retrieving KV blocks once attention mass hits a target (e.g., 95%).</li>
                    <li><strong>Unified KV Pool:</strong> Shares memory across layers to prevent fragmentation.</li>
                    <li><strong>Result:</strong> Up to 2.4x reduction in cache size with negligible accuracy loss.</li>
                </ul>
            </div>
        </div>
    </div>
</div>

<!-- Slide 6: HyperAttention -->
<div class="slide-container" id="slide6">
    <h2 class="slide-title">HyperAttention: Algorithmic Shift</h2>
    <div class="content-area">
        <div class="two-column">
            <div>
                <h3>Near-Linear Time Complexity</h3>
                <p>Identifies "heavy hitters" in the attention matrix without computing the full <math><mi>N</mi><mo>×</mo><mi>N</mi></math>.</p>
                <ul>
                    <li><strong>LSH Clustering:</strong> Hashes keys/queries to group similar vectors (large dot products).</li>
                    <li><strong>Sort & Block:</strong> Permutes the matrix to concentrate mass on the diagonal.</li>
                    <li><strong>Approximation:</strong> Exact compute for heavy blocks; random sampling for the residual.</li>
                </ul>
            </div>
            <!-- DIAGRAM: LSH Bucketing -->
            <div class="diagram-wrapper">
                <svg viewBox="0 0 400 300" width="100%" height="100%" style="font-family: var(--font-mono);">
                    <!-- Random Points -->
                    <circle cx="50" cy="50" r="4" fill="#94a3b8"/>
                    <circle cx="80" cy="120" r="4" fill="#94a3b8"/>
                    <circle cx="30" cy="200" r="4" fill="#94a3b8"/>
                    <text x="40" y="280" fill="#94a3b8" font-size="12">Input Vectors</text>

                    <!-- Hashing Function Arrow -->
                    <path d="M 100 150 L 150 150" stroke="#fff" stroke-width="2" marker-end="url(#arrowhead)"/>
                    <text x="110" y="140" fill="#fff" font-size="12">Hash</text>

                    <!-- Buckets -->
                    <rect x="180" y="50" width="60" height="200" fill="#1e293b" stroke="#f472b6" stroke-width="2"/>
                    <circle cx="210" cy="80" r="4" fill="#f472b6"/>
                    <circle cx="210" cy="100" r="4" fill="#f472b6"/>
                    <text x="190" y="270" fill="#f472b6" font-size="12">Bucket 1</text>
                    
                    <rect x="260" y="50" width="60" height="200" fill="#1e293b" stroke="#38bdf8" stroke-width="2"/>
                    <circle cx="290" cy="150" r="4" fill="#38bdf8"/>
                    <circle cx="290" cy="170" r="4" fill="#38bdf8"/>
                    <text x="270" y="270" fill="#38bdf8" font-size="12">Bucket 2</text>
                    
                    <text x="220" y="30" fill="#fff" font-size="14" text-anchor="middle">Similar Vectors Grouped</text>
                </svg>
            </div>
        </div>
    </div>
</div>

<!-- Slide 7: LoRA-Sparse -->
<div class="slide-container" id="slide7">
    <div class="content-area">
        <h2 class="slide-title">LoRA-Sparse: Learned Efficiency</h2>
        <div class="two-column">
             <!-- DIAGRAM: Low Rank Decomposition -->
             <div class="diagram-wrapper">
                <svg viewBox="0 0 400 300" width="100%" height="100%" style="font-family: var(--font-mono);">
                   <!-- Large Matrix W -->
                   <rect x="20" y="50" width="100" height="200" fill="none" stroke="#94a3b8" stroke-width="2" stroke-dasharray="5"/>
                   <text x="70" y="150" fill="#94a3b8" text-anchor="middle">Dense Attn</text>
                   
                   <text x="140" y="150" fill="#fff" font-size="24">=</text>
                   
                   <!-- A Matrix (Tall) -->
                   <rect x="170" y="50" width="40" height="200" fill="#38bdf8" opacity="0.8"/>
                   <text x="190" y="270" fill="#38bdf8" text-anchor="middle" font-size="12">N x r</text>

                   <text x="230" y="150" fill="#fff" font-size="24">×</text>

                   <!-- B Matrix (Wide) -->
                   <rect x="250" y="120" width="100" height="40" fill="#f472b6" opacity="0.8"/>
                   <text x="300" y="180" fill="#f472b6" text-anchor="middle" font-size="12">r x N</text>

                   <text x="200" y="30" fill="#fff" font-size="14" text-anchor="middle">Low-Rank Proxy (r << d)</text>
                </svg>
            </div>
            <div>
                <h3>Learned Sparsity Prediction</h3>
                <p>Uses low-rank adapters to <em>predict</em> where sparsity should occur.</p>
                <ul>
                    <li><strong>Low-Rank Proxies:</strong> Adds trainable <math><msub><mi>W</mi><mi>q</mi></msub></math>, <math><msub><mi>W</mi><mi>k</mi></msub></math> to map inputs to a small latent space.</li>
                    <li><strong>Order-Mimic Loss:</strong> Trains proxies to match the top-k ranking of the full attention map.</li>
                    <li><strong>Benefit:</strong> Can be retrofitted onto pre-trained dense models via fine-tuning.</li>
                </ul>
            </div>
        </div>
    </div>
</div>

<!-- Slide 8: Comparative Analysis -->
<div class="slide-container" id="slide8">
    <h2 class="slide-title">Strategic Trade-offs</h2>
    <div class="content-area">
        <table>
            <thead>
                <tr>
                    <th>Method</th>
                    <th>Philosophy</th>
                    <th>Optimization Target</th>
                    <th>Outcome</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>FlashAttention-3</strong></td>
                    <td>Exact / Hardware</td>
                    <td>Compute Throughput</td>
                    <td>75% of H100 Peak</td>
                </tr>
                <tr>
                    <td><strong>PSA</strong></td>
                    <td>Heuristic Sparsity</td>
                    <td>Inference Memory</td>
                    <td>2.4x Cache Reduction</td>
                </tr>
                <tr>
                    <td><strong>HyperAttention</strong></td>
                    <td>Randomized Algo</td>
                    <td>Asymptotic Scaling</td>
                    <td><math><mi>O</mi><mfenced><mrow><mi>N</mi><mi>log</mi><mi>N</mi></mrow></mfenced></math> Time</td>
                </tr>
                <tr>
                    <td><strong>LoRA-Sparse</strong></td>
                    <td>Learned / Low-Rank</td>
                    <td>Model Adaptation</td>
                    <td>Retrofitted Sparsity</td>
                </tr>
            </tbody>
        </table>
    </div>
</div>

<!-- Slide 9: Performance Gains -->
<div class="slide-container" id="slide9">
    <h2 class="slide-title">Performance Impact (Based on Paper Data)</h2>
    <div class="content-area">
        <div class="chart-container">
            <!-- FlashAttention-3 -->
            <div class="bar-row">
                <div class="bar-label">FlashAttention-3</div>
                <div class="bar-track">
                    <div class="bar-fill" style="width: 40%;">Up to 2.0x (vs FP16)</div>
                </div>
            </div>
            <!-- PSA Throughput -->
            <div class="bar-row">
                <div class="bar-label">PSA (Throughput)</div>
                <div class="bar-track">
                    <div class="bar-fill" style="width: 35%;">1.4x - 2.0x Increase</div>
                </div>
            </div>
             <!-- PSA Cache -->
             <div class="bar-row">
                <div class="bar-label">PSA (KV Cache Size)</div>
                <div class="bar-track">
                    <div class="bar-fill" style="width: 48%;">2.4x Reduction</div>
                </div>
            </div>
            <!-- HyperAttention -->
            <div class="bar-row">
                <div class="bar-label">HyperAttention (Causal)</div>
                <div class="bar-track">
                    <div class="bar-fill" style="width: 80%;">Up to 5.0x Speedup</div>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Slide 10: Future Directions -->
<div class="slide-container" id="slide10">
    <h2 class="slide-title">Open Questions & Future Directions</h2>
    <div class="content-area">
        <div class="styled-list" style="max-width: 900px;">
            <ul>
                <li><strong>Hardware-Sparsity Co-design:</strong> Need for native GPU primitives that handle irregular block-sparse operations efficiently.</li>
                <li><strong>Synergies:</strong> Combining FlashAttention's IO-kernels with PSA's sparsity logic.</li>
                <li><strong>Energy Efficiency:</strong> Standardizing metrics for FLOPs/Watt in sustainable AI.</li>
                <li><strong>Beyond Attention:</strong> Optimizing Feed-Forward Networks (FFNs), which account for ~60% of total compute.</li>
            </ul>
        </div>
    </div>
</div>

<!-- Slide 11: Conclusion -->
<div class="slide-container full-bg-slide" id="slide11">
    <div class="full-bg-overlay">
        <h2 style="font-size: 56px; color: #fff; margin-bottom: 30px;">Conclusion</h2>
        <p style="color: #e2e8f0; font-size: 26px;">The path to scalable long-context LLMs requires a convergence of hardware-aware kernels, intelligent sparsity, and algorithmic approximation.</p>
    </div>
</div>

<!-- Slide 12: Q&A -->
<div class="slide-container" id="slide12">
    <div class="content-area" style="text-align: center;">
        <h2 style="font-size: 80px; color: var(--accent-primary); margin-bottom: 40px;">Q&A</h2>
        <p style="font-size: 32px; margin-bottom: 60px;">Thank you for your attention.</p>
    </div>
</div>

<script>
    // --- Slide Navigation Logic ---
    let currentSlideIndex = 1;
    const totalSlides = 12;

    function showSlide(index) {
        // Hide all slides
        document.querySelectorAll('.slide-container').forEach(slide => {
            slide.classList.remove('active');
        });

        // Loop checks
        if (index > totalSlides) currentSlideIndex = 1;
        if (index < 1) currentSlideIndex = totalSlides;

        // Show active slide
        const activeSlide = document.getElementById('slide' + currentSlideIndex);
        if (activeSlide) {
            activeSlide.classList.add('active');
        }
        
        // Update Counter
        document.getElementById('counter').textContent = `${currentSlideIndex} / ${totalSlides}`;
    }

    function nextSlide() {
        currentSlideIndex++;
        showSlide(currentSlideIndex);
    }

    function prevSlide() {
        currentSlideIndex--;
        showSlide(currentSlideIndex);
    }

    function toggleFullScreen() {
        if (!document.fullscreenElement) {
            document.documentElement.requestFullscreen();
        } else {
            if (document.exitFullscreen) {
                document.exitFullscreen();
            }
        }
    }

    // Keyboard controls
    document.addEventListener('keydown', (e) => {
        if (e.key === 'ArrowRight' || e.key === ' ') {
            nextSlide();
        } else if (e.key === 'ArrowLeft') {
            prevSlide();
        } else if (e.key === 'f') {
            toggleFullScreen();
        }
    });

    // Initialize
    showSlide(1);
</script>

</body>
</html>
